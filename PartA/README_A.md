# Parte A ‚Äî Generaci√≥n de letras de canciones

Esta parte del proyecto se centra en la **generaci√≥n de texto** (letras de canciones) usando modelos RNN, LSTM, GRU y Transformers.



# Parte A ‚Äî Generaci√≥n de letras de canciones

Esta parte implementa **modelos de generaci√≥n de texto** (letras de canciones) en espa√±ol utilizando dos familias de arquitecturas:

1. **Modelos cl√°sicos:** RNN, LSTM y GRU (a nivel car√°cter y palabra).  
2. **Modelos Transformer:** LLaMA 3 + LoRA (fine-tuning 4-bit).



## üìÇ Estructura

```bash
PartA/
‚îú‚îÄ‚îÄ data/           # Canciones limpias y splits (train/valid)
‚îú‚îÄ‚îÄ src/            # C√≥digo fuente (preprocesamiento, entrenamiento, generaci√≥n)
‚îú‚îÄ‚îÄ models/         # Modelos entrenados y checkpoints
‚îú‚îÄ‚îÄ results/        # M√©tricas y gr√°ficas
‚îú‚îÄ‚îÄ notebooks/      # Notebooks exploratorios y de pruebas
‚îú‚îÄ‚îÄ logs/           # Logs de ejecuci√≥n SLURM
‚îú‚îÄ‚îÄ run_textgen.sh          # Entrenamiento cl√°sico (RNN/LSTM/GRU)
‚îú‚îÄ‚îÄ run_train_LlaMA.sh      # Fine-tuning LLaMA 3 + LoRA
‚îî‚îÄ‚îÄ run_generate_Llama.sh   # Generaci√≥n con modelo LLaMA 3
```

## Dependencias y entorno

Todas las librer√≠as necesarias est√°n especificadas en el archivo environment.yml.
Para crear el entorno (en local o en el cl√∫ster):

```bash
conda env create -f environment.yml
conda activate tarea2-nlp
```

## Flujo de trabajo

1Ô∏è‚É£ Descarga de canciones

Usa la API de Genius para descargar ‚â• 100 letras:

```bash
python src/extraer_canciones.py
```
2Ô∏è‚É£ Limpieza y preprocesamiento
Genera `data/canciones_clean.txt` libre de metadatos y ruido.

```bash
python src/limpiar_canciones.py
```
Opcionalmente, analiza estadisticas del corpus:

```bash
python src/song_statistics.py
```
Genera histogramas en `results/figures/hist_longitudes.png`.



3Ô∏è‚É£ **Conversi√≥n a JSONL (para Transformers)**

Convierte el corpus limpio a formato JSONL, necesario para entrenar modelos tipo Transformer:

```bash
python src/convert_to_train_lyrics.py
```

Esto crea `data/train_lyrics.jsonl`, con una canci√≥n por l√≠nea (clave `"text"`).

---

4Ô∏è‚É£ **Entrenamiento de modelos cl√°sicos (RNN/LSTM/GRU)**

**Ejecuci√≥n local** (ejemplo: GRU a nivel palabra):

```bash
python -u src/train_textgen.py \
  --arch gru --level word --epochs 30 \
  --hidden_size 128 --embedding_dim 256 \
  --lr 5e-5 --batch_size 64 --seq_len 20 \
  --data_path data/canciones_clean.txt \
  --save_dir models/ --results_dir results/ \
  --mode train
```

**Ejecuci√≥n en cl√∫ster (Lab-SB / CIMAT):**

```bash
sbatch run_textgen.sh gru word 2 128 0.2 30 64 20 256 5e-5 data/canciones_clean.txt models/ results/
```

Los logs se guardan en `logs/textgen-<jobid>.log` y los modelos/m√©tricas en `models/gru_word/`.

---

5Ô∏è‚É£ **Generaci√≥n de letras (con modelos cl√°sicos)**

Para generar m√∫ltiples combinaciones de prompt y temperatura:

```bash
python src/batch_generator.py
```

Esto genera archivos `.txt` y metadatos `.json` en `results/batch/`.

---

6Ô∏è‚É£ **Fine-tuning de LLaMA 3 + LoRA (Transformers)**

El modelo base debe estar cacheado en `models/Meta-Llama-3-8B/`.

**Ejecuci√≥n local:**

```bash
python -u src/train_llama.py \
  --model_path models/Meta-Llama-3-8B/snapshots/.../ \
  --data_path data/train_lyrics.jsonl \
  --run_name llama3_v4 --epochs 5 \
  --batch_size 2 --block_size 256 \
  --lr 1e-4 --dropout 0.2
```

**Ejecuci√≥n en cl√∫ster:**

```bash
sbatch run_train_LlaMA.sh llama3_v4 5 2 256 1e-4 0.2
```

Los logs se guardan en `logs/llama3-<jobid>.log` y el modelo fine-tuneado en `models/llama3_v4/`.

---

7Ô∏è‚É£ **Generaci√≥n de letras con LLaMA 3 fine-tuneado**

**Ejecuci√≥n local:**

```bash
python -u src/generate_llama.py \
  --model_path results/llama3_v4 \
  --prompt "En la penumbra del d√≠a" \
  --max_new_tokens 200 --temperature 1.0 --top_p 0.95 \
  --output_dir results/LLaMA
```

**Ejecuci√≥n en cl√∫ster:**

```bash
sbatch run_generate_Llama.sh "Bailando bajo la lluvia" 200 1.0 0.95 llama3_out
```

Esto genera `.txt` y `.json` con los par√°metros en `results/llama3_out/`.


# üîÅ Reproducibilidad

- Semilla global fijada a 42 en todos los scripts.
- Logs de entrenamiento y configuraciones guardados en .csv y .json.
- Todos los scripts aceptan par√°metros por l√≠nea de comando (argparse).
- Directorios de salida autom√°ticamente creados.
- Compatible con CPU y GPU (Titan RTX, 24 GB VRAM probado en CIMAT).


# Autor

**Uziel Luj√°n**

**Maestr√≠a en C√≥mputo Estad√≠stico ‚Äî CIMAT**
